<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Final Project - Sentiment Analysis</title>
    <link rel="stylesheet" href="style-page.css">
</head>
<body>
    <h1>Final Project Report</h1>

    <h2>Introduction</h2>
    <p>
       In this project, I set out to develop a sentiment analysis tool capable of classifying movie reviews as either positive or negative. To achieve this, I leveraged Natural Language Processing techniques and a pre-trained BERT model. The primary objective was to fine-tune the model for high accuracy while maintaining reliability and scalability. My vision for this project was to explore practical applications of state of the art NLP technologies and to deepen my understanding of how they can be adapted to solve real world problems. Along the way, I aimed to create a user friendly demo that showcased the model’s predictive capabilities in an interactive and accessible format.
    </p>

    <h2>Background</h2>
    <p>
        Sentiment analysis has become an invaluable tool across a wide range of industries, from customer service to entertainment, allowing organizations to derive meaningful insights from textual data. This project resonated with me because it merges my academic background in computer science with my passion for exploring emerging technologies like artificial intelligence. Professionally, mastering Natural Language Processing is essential for contributing to innovative AI-driven projects, particularly those that rely on understanding and processing human language effectively.
    </p>
    <p>
        This project also provided a unique opportunity to connect theoretical knowledge with practical applications. By working with real-world data and creating a fully functional demo, I was able to gain hands-on experience that directly aligns with the demands of roles in the AI and technology sectors. The process of translating concepts into actionable outcomes not only enhanced my technical proficiency but also reinforced my confidence in applying advanced AI techniques to solve practical problems. Additionally, it underscored the significance of sentiment analysis in real-world scenarios, such as analyzing customer feedback or evaluating product reviews, where understanding sentiment plays a pivotal role in decision-making.
    </p>

    <h2>Methodology, Materials, and Methods</h2>
    <p>
        This project relied on several tools and methodologies to support its development and execution. The Hugging Face Transformers library, combined with PyTorch and TensorFlow, was integral to working with the BERT model. I utilized the IMDb movie review dataset, which contains labeled positive and negative reviews, as the foundation for training and evaluating the model. The pre-trained BERT model I selected was google-bert, which I fine-tuned for sentiment classification before uploading the optimized version to Hugging Face.
    </p>
    <p>
        The bulk of the development work was conducted in Jupyter Notebook, which provided an efficient platform for experimentation and visualization during the model fine-tuning process. To make the project accessible to users, I created an interactive demo hosted on Hugging Face Spaces, leveraging its built-in Gradio interface. This demo allows users to input movie reviews and receive sentiment predictions in real time. The interface highlights the predictive capabilities of the model in an intuitive and user-friendly format, serving as a practical example of how advanced AI tools can be deployed for everyday applications.
    </p>
    <p>
        For version control and documentation, I used GitHub as the central repository, ensuring that every iteration of the project was properly tracked. Visual Studio Code served as my primary code editor, offering a streamlined environment for writing, testing, and refining code. Evaluation metrics such as accuracy, precision, recall, and F1-score were employed to gauge the model's performance and guide iterative improvements throughout the process.
    </p>
    <p>
        My workflow involved several critical steps, including setting up the environment, exploring the dataset, fine-tuning the model, and optimizing hyperparameters to achieve the desired performance. Along the way, I meticulously documented the workflow, recorded challenges, and the solutions implemented to address them. The accompanying website further supports the project by presenting the technical details and providing a platform for interactive exploration, making the work accessible to both technical and non-technical audiences.
    </p>

    <h2>Results</h2>
    <p>
        The fine-tuned BERT model demonstrated strong performance, achieving an accuracy of 92%. This result highlights its ability to effectively classify sentiments in IMDb movie reviews, reliably distinguishing between positive and negative sentiments. This performance not only validated the model's design but also reinforced the viability of transformer-based architectures for real-world NLP tasks. Throughout the project, I gained a deeper understanding of transformer models, the nuances of hyperparameter tuning, and the inherent challenges of fine-tuning NLP models. Importantly, the project met all initial goals, achieving accuracy benchmarks and successfully showcasing the results through an interactive demo.
    </p>
    <p>
        In addition to reaching these technical objectives, I developed key skills that are critical for future work. I significantly improved my ability to preprocess data, fine-tune advanced models, and create practical demonstrations of technical achievements. Deploying the model through a web interface was another milestone, as it required bridging the gap between machine learning development and user accessibility. This aspect of the project taught me the importance of presenting predictive results in a clear and interactive format, which proved to be both practical and impactful.
    </p>
    <p>
        The model exhibited balanced performance across positive and negative classes, as evidenced by its metrics. For the positive class, the model showed high precision, meaning predictions for positive sentiment were highly reliable with minimal false positives. For the negative class, the model achieved a recall of 0.96, ensuring that it captured most negative reviews accurately. These results were complemented by balanced F1-scores of 0.93 for negative and 0.92 for positive, indicating a strong equilibrium between precision and recall across both classes.
    </p>
    <p>
        Despite its strengths, the model faced some challenges when handling misclassifications. Reviews with mixed or subtle sentiments, such as “The movie was well-acted, but the story was dull,” were occasionally misinterpreted due to their nuanced tone. Similarly, sarcasm and complex sentence structures, like “What a masterpiece… not!,” posed significant difficulties for the model. Another issue stemmed from data imbalance in vocabulary, where certain rare words heavily associated with one sentiment skewed predictions, leading to occasional false positives or negatives.
    </p>
    <p>
        These challenges offered valuable insights into areas where the model could be improved. Expanding the dataset to include more diverse examples of subtle or complex language could enhance the model’s robustness. Additionally, exploring advanced techniques for handling nuanced expressions, such as sentiment-specific fine-tuning or incorporating contextual embeddings, could further refine its accuracy. These findings underline the iterative nature of machine learning and the ongoing opportunities for improvement even in high-performing models.
    </p>

    <h2>Discussion / Reflection</h2>
    <p>
        This project successfully achieved its goals, thanks to weekly planning from this course, thorough research, and iterative testing. A key factor in its success was my ability to effectively leverage existing resources while adapting them to meet the unique needs of the project. One of the most significant challenges was understanding the nuances of fine-tuning and managing the computational demands of working with large NLP models like BERT. To address these issues, I utilized cloud resources and sought insights from online communities, which proved invaluable. However, in hindsight, a more structured plan for hardware allocation and resource management could have further streamlined the process and saved time.
    </p>
    <p>
        Fine-tuning the BERT model was undoubtedly one of the most time-intensive aspects of the project, requiring more than approximately 40 hours to complete. The computational demands of the model and the iterative adjustments needed to optimize performance made this phase particularly challenging. To ensure efficiency and mitigate potential setbacks, I implemented a checkpointing code. This approach allowed me to save the model's progress at regular intervals, ensuring that training could resume seamlessly if interrupted. This code was critical for maintaining momentum, as it prevented the loss of progress and minimized downtime, saving significant time and effort throughout the process.
    </p>
    <p>
        If I were to approach the project again, I would dedicate additional time to exploring diverse datasets to enhance the model's applicability to a broader range of contexts. Despite this, I am good with the project’s results, particularly the interactive demo, which provides a tangible, user-friendly interface for showcasing the model’s capabilities.
    </p>
    <p>
        Reflecting on this experience, I have gained a deeper appreciation for the importance of balancing technical depth with practical implementation. Managing time and resources effectively throughout the project has significantly boosted my confidence in tackling future challenges in the tech field. Overall, this project reinforced the value of aligning theoretical knowledge with practical applications to create impactful and accessible solutions.
    </p>

    <h2>Conclusion & Future Work</h2>
    <p>
        Looking back, this project confirmed my interest in NLP. At present, I feel equipped with a robust foundational understanding of sentiment analysis and transformer models. Moving forward, I plan to expand the project to include multilingual sentiment analysis and explore additional NLP applications such as summarization and question answering.
    </p>
    <p>
        Despite strong results, there are areas for improvement. The model struggles with reviews that contain mixed or subtle emotions, such as sarcasm or complex expressions. Additionally, since the model was trained on movie reviews, it would benefit from training on reviews from other domains, such as product or social media comments, to enhance generalization. Computational efficiency is another area for improvement, as BERT is computationally intensive. Exploring lighter models like DistilBERT could make the model more efficient for real-time use.
    </p>
    <p>
        To address these limitations, future work could focus on experimenting with alternative transformer models like RoBERTa and DistilBERT for improved efficiency. Fine-tuning on a multi-domain dataset could enhance generalization, and exploring multi-class sentiment analysis could capture a broader range of emotions. This project has been a deeply rewarding experience, and I’m excited to take the lessons I’ve learned here into future endeavors.
    </p>

    <h2>References or Bibliography</h2>
    <ul>
        <li><a href="https://huggingface.co/transformers/">Hugging Face Transformers Library</a></li>
        <li><a href="https://ai.stanford.edu/~amaas/data/sentiment/">IMDb Large Movie Review Dataset</a></li>
        <li><a href="https://huggingface.co/google-bert/bert-base-uncased">Pre-trained BERT model</a></li>
        <li><a href="https://github.com/Blackjack007H/BlackJack007">Project Repository on GitHub</a></li>
        <li><a href="https://huggingface.co/spaces/blackjack007007/MovieReviewSentiment">Hugging Face Spaces Demo</a></li>
        <li><a href="https://pytorch.org/docs/">PyTorch Documentation</a></li>
        <li><a href="https://www.tensorflow.org">TensorFlow Documentation</a></li>
    </ul>

    <a href="index.html" class="button">Back to Homepage</a>
</body>
</html>
